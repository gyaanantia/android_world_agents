#!/usr/bin/env python3
"""
Comprehensive test of Text2Grad integration for AndroidWorld agents.

This test validates the complete Text2Grad optimization cycle:
1. Snapshot creation and restoration
2. Gemini visual analysis generation  
3. Text2Grad optimization rollouts with dense rewards
4. Environment restoration and optimized step execution
"""

import sys
import os
import logging
from unittest.mock import Mock, MagicMock
import numpy as np

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.text2grad_agent import Text2GradAgent, Text2GradConfig, Text2GradOptimizer
from src.gemini_prompting import GeminiPromptGenerator

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def create_mock_env():
    """Create a mock AndroidWorld environment."""
    env = Mock()
    env.controller = Mock()
    env.get_state = Mock()
    
    # Mock state with screenshot
    state = Mock()
    state.pixels = np.random.randint(0, 255, (1080, 720, 3), dtype=np.uint8)
    env.get_state.return_value = state
    
    return env

def create_mock_task():
    """Create a mock task evaluation."""
    task = Mock()
    task.goal = "Test task: Navigate to settings and enable dark mode"
    task.is_successful = Mock(return_value=0.8)  # 80% success
    return task

def create_mock_gemini_generator():
    """Create a mock Gemini prompt generator."""
    generator = Mock()
    generator.generate_agent_prompt = Mock(return_value={
        'success': True,
        'raw_response': 'The screen shows a main navigation interface with settings icon visible in the top-right corner. To access dark mode settings, tap the settings icon.',
        'agent_prompt': 'Navigate to settings by tapping the gear icon in the top-right corner of the screen.'
    })
    return generator

def test_text2grad_optimization_cycle():
    """Test the complete Text2Grad optimization cycle."""
    logger.info("ğŸ¯ Starting Text2Grad optimization cycle test...")
    
    # Step 1: Create test components
    logger.info("ğŸ“‹ Setting up test components...")
    env = create_mock_env()
    task = create_mock_task()
    gemini_generator = create_mock_gemini_generator()
    
    config = Text2GradConfig(
        k_rollouts=2,  # Reduced for testing
        n_steps=2,     # Reduced for testing
        use_original_text2grad=False,  # Use simplified implementation
        enable_early_stopping=False    # Test all rollouts
    )
    
    # Step 2: Create Text2Grad agent
    logger.info("ğŸš€ Creating Text2Grad agent...")
    agent = Text2GradAgent(
        env=env,
        model_name="gpt-4o-mini",
        text2grad_config=config,
        gemini_generator=gemini_generator
    )
    
    # Step 3: Mock the base agent's step method
    logger.info("ğŸ”§ Setting up agent mocks...")
    step_results = []
    for i in range(4):  # Enough for 2 rollouts of 2 steps each
        result = Mock()
        result.done = i >= 1  # Done after first step in each rollout
        result.data = {'action_output': {'action': f'test_action_{i}', 'step': i}}
        step_results.append(result)
    
    agent.agent.step = Mock(side_effect=step_results)
    
    # Step 4: Test the Text2Grad optimization step
    logger.info("ğŸ¯ Testing Text2Grad optimization step...")
    
    try:
        # Execute one Text2Grad optimization step
        result = agent.step(goal=task.goal, task=task)
        
        # Validate the results
        logger.info("âœ… Text2Grad step completed successfully!")
        
        # Check that Gemini was called
        assert gemini_generator.generate_agent_prompt.called, "âŒ Gemini generator not called"
        logger.info("âœ… Gemini visual analysis was generated")
        
        # Check that optimization history was recorded
        assert len(agent.optimization_history) == 1, "âŒ Optimization history not recorded"
        logger.info("âœ… Optimization history recorded")
        
        # Check optimization results
        opt_results = agent.optimization_history[0]['optimization_results']
        assert opt_results['status'] == 'completed', "âŒ Optimization not completed"
        assert opt_results['rollouts_completed'] >= 1, "âŒ No rollouts completed"
        logger.info(f"âœ… Optimization completed: {opt_results['rollouts_completed']} rollouts")
        
        # Check that Text2Grad metadata was added
        assert result.data['text2grad_optimization'], "âŒ Text2Grad metadata missing"
        assert result.data['optimized_prompt_used'], "âŒ Optimized prompt flag missing"
        logger.info("âœ… Text2Grad metadata properly attached")
        
        # Get optimization summary
        summary = agent.get_optimization_summary()
        logger.info(f"ğŸ“Š Optimization Summary:")
        logger.info(f"  â€¢ Total optimizations: {summary['total_optimizations']}")
        logger.info(f"  â€¢ Success rate: {summary['success_rate']:.2%}")
        logger.info(f"  â€¢ Average improvement: {summary['average_improvement']:.3f}")
        
        logger.info("ğŸ‰ Text2Grad optimization cycle test PASSED!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Text2Grad optimization cycle test FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_text2grad_optimizer_directly():
    """Test the Text2GradOptimizer component directly."""
    logger.info("ğŸ”¬ Testing Text2GradOptimizer directly...")
    
    try:
        # Create test components
        env = create_mock_env()
        task = create_mock_task()
        gemini_generator = create_mock_gemini_generator()
        
        config = Text2GradConfig(k_rollouts=2, n_steps=2, use_original_text2grad=False)
        base_agent_config = {'model_name': 'gpt-4o-mini', 'prompt_variant': 'base'}
        
        # Create optimizer
        optimizer = Text2GradOptimizer(
            config=config,
            gemini_generator=gemini_generator,
            base_agent_config=base_agent_config
        )
        
        # Test prompt perturbation
        base_prompt = "Original prompt for UI analysis"
        perturbed = optimizer._perturb_gemini_prompt(base_prompt, 0)
        assert len(perturbed) > len(base_prompt), "âŒ Prompt perturbation failed"
        logger.info("âœ… Prompt perturbation working")
        
        # Mock snapshot manager
        optimizer.snapshot_manager.save_snapshot = Mock()
        optimizer.snapshot_manager.restore_snapshot = Mock()
        
        # Test optimization (would need full agent mock for complete test)
        logger.info("âœ… Text2GradOptimizer structure validated")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Text2GradOptimizer test FAILED: {e}")
        return False

if __name__ == "__main__":
    logger.info("ğŸ§ª Starting comprehensive Text2Grad integration test...")
    
    test1_passed = test_text2grad_optimizer_directly()
    test2_passed = test_text2grad_optimization_cycle()
    
    if test1_passed and test2_passed:
        logger.info("ğŸ‰ ALL TESTS PASSED! Text2Grad integration is working correctly!")
        logger.info("ğŸš€ Ready for real AndroidWorld episode testing!")
    else:
        logger.error("âŒ Some tests failed. Review the implementation.")
        sys.exit(1)
